# Experiment Configuration for Lottery Ticket Hypothesis Pruning Study
# This config covers comparative study of pruning methods and adaptive hybrid pruning

experiment:
  name: "LTH Pruning Experiments"
  description: "Comparative study and improvement of pruning methods related to Lottery Ticket Hypothesis"

# Datasets to use
datasets:
  - name: cifar10
    num_classes: 10
  - name: cifar100
    num_classes: 100

# Model architectures
models:
  - resnet20
  # - resnet50
  # - vgg16

# Pruning methods (baselines and proposed)
pruning_methods:
  - imp  # Iterative Magnitude Pruning
  - grasp  # Gradient Signal Preservation
  - synflow  # Synaptic Flow
  - early_bird  # Early-Bird Ticket
  - genetic  # Genetic Algorithm Pruning
  - hybrid  # Hybrid OneShot-Iterative
  - adaptive_hybrid  # Proposed: Adaptive Hybrid with Dynamic Switching

# Training hyperparameters
training:
  epochs: 160  # Standard for CIFAR
  batch_size: 128
  learning_rate: 0.1
  momentum: 0.9
  weight_decay: 5e-4
  scheduler: cosine  # Learning rate scheduler
  warmup_epochs: 5

# Pruning configuration
pruning:
  sparsity_levels: [0.3, 0.6]  # Target sparsities
  pruning_schedule: iterative  # or oneshot
  iterations: 5  # For iterative pruning
  adaptive_threshold: true  # For proposed method

# Genetic Algorithm configuration
genetic_algorithm:
  population_size: 100
  rec_rate: 0.3          # Recombination rate
  mut_rate: 0.1          # Mutation rate
  mig_rate: 0.1          # Migration rate (fraction of pop replaced by random)
  par_rate: 0.3          # Top fraction eligible for mating
  min_generations: 100   # Minimum GA generations before stagnation check
  max_generations: 200   # Hard cap on generations
  stagnation_threshold: 50  # Stop after N gens without improvement
  use_adaptive_ab: false    # Adaptive accuracy-bound initialisation
  initial_ab_threshold: 0.7 # Initial accuracy bound (if adaptive AB enabled)
  ab_decay_rate: 0.95       # Exponential decay for accuracy bound
  use_loss_fitness: true    # Use negative loss as fitness (true) vs accuracy (false)
  max_eval_batches: null    # Limit batches per evaluation (null = all data)
  post_prune: true          # Post-evolutionary pruning for extra sparsity

# Experiment settings
experiment_settings:
  seeds: [42]  # 3 seeds as per plan
  num_runs: 3  # Minimum runs per config
  save_checkpoints: true
  log_interval: 100  # Steps between logs

# Evaluation metrics
evaluation:
  metrics: [accuracy, loss, sparsity, flops, params]
  test_frequency: 5  # Evaluate every 5 epochs

# Logging and tracking
logging:
  wandb:
    enabled: true
    project: "lth-pruning-study"
    entity: null  # Set your wandb entity
  tensorboard: true
  save_dir: "./results"

# Hardware
hardware:
  device: cuda  # or cpu
  num_workers: 4
  pin_memory: true