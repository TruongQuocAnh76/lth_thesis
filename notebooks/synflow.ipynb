{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d431b5",
   "metadata": {},
   "source": [
    "# SynFlow Experiment — ResNet-20 on CIFAR-10 & CIFAR-100\n",
    "\n",
    "Reproducing iterative data-free pruning from *Pruning neural networks without any data by iteratively conserving synaptic flow* (Tanaka et al., NeurIPS 2020).\n",
    "\n",
    "| Setting | Value |\n",
    "|---|---|\n",
    "| **Architecture** | ResNet-20 (CIFAR variant) |\n",
    "| **Datasets** | CIFAR-10, CIFAR-100 |\n",
    "| **Initialization** | Kaiming Normal |\n",
    "| **Pruning** | SynFlow — 100 iterations, exponential schedule, global scope, data-free |\n",
    "| **Optimizer** | SGD, momentum 0.9, weight decay 1e-4 |\n",
    "| **Epochs** | 160 |\n",
    "| **Batch size** | 128 |\n",
    "| **Learning rate** | 0.1 → ×0.1 at epochs 80, 120 |\n",
    "| **Sparsities** | 30 %, 60 % (matching config) |\n",
    "\n",
    "We compare **Dense** (unpruned) vs SynFlow-pruned networks at each sparsity level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969681e",
   "metadata": {},
   "source": [
    "## 1 — Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, copy, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Project imports\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from model import resnet20, count_parameters\n",
    "from synflow import synflow_pruning, apply_synflow_masks, get_synflow_sparsity\n",
    "from train import train_epochs, evaluate\n",
    "from util import apply_masks_to_model, create_mask_apply_fn, set_seed\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8904f",
   "metadata": {},
   "source": [
    "## 2 — Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b1b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Model / datasets\n",
    "    'model':        'resnet20',\n",
    "    'datasets':     ['cifar10', 'cifar100'],\n",
    "\n",
    "    # SynFlow pruning\n",
    "    'sparsities':   [0.0, 0.30, 0.60],    # 0.0 = dense baseline\n",
    "    'synflow_iters': 100,                   # iterative pruning rounds\n",
    "    'input_shape':  (3, 32, 32),            # CIFAR spatial dims\n",
    "\n",
    "    # Training\n",
    "    'epochs':       160,\n",
    "    'batch_size':   128,\n",
    "    'lr':           0.1,\n",
    "    'momentum':     0.9,\n",
    "    'weight_decay': 1e-4,\n",
    "    'lr_milestones': [80, 120],\n",
    "    'lr_gamma':     0.1,\n",
    "\n",
    "    # Reproducibility\n",
    "    'seed':         42,\n",
    "\n",
    "    # Output\n",
    "    'results_dir':  '../results/synflow',\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k:>20s}: {v}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d2a7aa",
   "metadata": {},
   "source": [
    "## 3 — Dataset helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e3493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(dataset_name, batch_size=128):\n",
    "    \"\"\"Return train_loader, test_loader, num_classes for a CIFAR dataset.\"\"\"\n",
    "    if dataset_name == 'cifar10':\n",
    "        mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        num_classes = 10\n",
    "        DS = torchvision.datasets.CIFAR10\n",
    "    else:\n",
    "        mean, std = (0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)\n",
    "        num_classes = 100\n",
    "        DS = torchvision.datasets.CIFAR100\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    train_ds = DS(root='../data', train=True,  download=True, transform=train_transform)\n",
    "    test_ds  = DS(root='../data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=256,\n",
    "                              shuffle=False, num_workers=2, pin_memory=True)\n",
    "    print(f\"[{dataset_name}] Train: {len(train_ds):,}  Test: {len(test_ds):,}  Classes: {num_classes}\")\n",
    "    return train_loader, test_loader, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483de0ec",
   "metadata": {},
   "source": [
    "## 4 — Helper: train one configuration end-to-end\n",
    "\n",
    "Encapsulates: model init → (optional) SynFlow pruning → full 160-epoch training.\n",
    "Time for both pruning and training is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single(dataset_name: str, sparsity: float, config: dict):\n",
    "    \"\"\"Create model, optionally apply SynFlow, then train for 160 epochs.\n",
    "\n",
    "    Returns a dict with training history, final accuracy, mask info, and timing.\n",
    "    \"\"\"\n",
    "    set_seed(config['seed'])\n",
    "\n",
    "    train_loader, test_loader, num_classes = get_loaders(dataset_name, config['batch_size'])\n",
    "\n",
    "    # ---- Model (Kaiming Normal init) ----\n",
    "    model = resnet20(num_classes=num_classes).to(device)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    params = count_parameters(model)\n",
    "    tag = \"dense\" if sparsity == 0.0 else f\"{sparsity*100:.0f}%\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{dataset_name} / {tag}]  sparsity = {sparsity*100:.0f}%\")\n",
    "    print(f\"  Total params: {params['total']:,}\")\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    # ---- SynFlow pruning (skip for dense baseline) ----\n",
    "    masks = None\n",
    "    apply_fn = None\n",
    "    prune_time = 0.0\n",
    "    layer_sparsity = {}\n",
    "\n",
    "    if sparsity > 0:\n",
    "        print(f\"  Running SynFlow ({config['synflow_iters']} iters, data-free) …\")\n",
    "        prune_start = time.time()\n",
    "\n",
    "        masks = synflow_pruning(\n",
    "            model,\n",
    "            device=device,\n",
    "            target_sparsity=sparsity,\n",
    "            num_iters=config['synflow_iters'],\n",
    "            input_shape=config['input_shape'],\n",
    "        )\n",
    "        prune_time = time.time() - prune_start\n",
    "        print(f\"  SynFlow pruning completed in {prune_time:.2f}s\")\n",
    "\n",
    "        # Apply masks in-place\n",
    "        apply_synflow_masks(model, masks)\n",
    "\n",
    "        layer_sparsity = get_synflow_sparsity(masks)\n",
    "        print(f\"  Achieved overall sparsity: {layer_sparsity['overall']*100:.2f}%\")\n",
    "        for name, sp in layer_sparsity.items():\n",
    "            if name != 'overall':\n",
    "                print(f\"    {name:>30s}: {sp*100:.2f}%\")\n",
    "\n",
    "        apply_fn = create_mask_apply_fn(model)\n",
    "\n",
    "    # ---- Optimizer / scheduler ----\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        momentum=config['momentum'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "    )\n",
    "    scheduler = MultiStepLR(optimizer,\n",
    "                            milestones=config['lr_milestones'],\n",
    "                            gamma=config['lr_gamma'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ---- Train ----\n",
    "    print(f\"  Training for {config['epochs']} epochs …\")\n",
    "    train_start = time.time()\n",
    "    history = train_epochs(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=config['epochs'],\n",
    "        device=device,\n",
    "        scheduler=scheduler,\n",
    "        masks=masks,\n",
    "        apply_mask_fn=apply_fn,\n",
    "        verbose=True,\n",
    "    )\n",
    "    train_time = time.time() - train_start\n",
    "    total_time = time.time() - total_start\n",
    "\n",
    "    best_test = max(history['test_accs'])\n",
    "    final_test = history['final_test_acc']\n",
    "    print(f\"  ✓ Done — best test acc: {best_test:.2f}%, \"\n",
    "          f\"final test acc: {final_test:.2f}%\")\n",
    "    print(f\"  Pruning: {prune_time:.1f}s | Training: {train_time:.1f}s | Total: {total_time:.1f}s\")\n",
    "\n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'sparsity': sparsity,\n",
    "        'history': history,\n",
    "        'best_test_acc': best_test,\n",
    "        'final_test_acc': final_test,\n",
    "        'masks': masks,\n",
    "        'layer_sparsity': layer_sparsity,\n",
    "        'pruning_time': prune_time,\n",
    "        'training_time': train_time,\n",
    "        'total_time': total_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bedf95",
   "metadata": {},
   "source": [
    "## 5 — Run experiments\n",
    "\n",
    "Train the dense baseline and each SynFlow sparsity level for **both** CIFAR-10 and CIFAR-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for dataset_name in config['datasets']:\n",
    "    all_results[dataset_name] = {}\n",
    "    for sparsity in config['sparsities']:\n",
    "        tag = \"dense\" if sparsity == 0.0 else f\"{sparsity*100:.0f}%\"\n",
    "        result = run_single(dataset_name, sparsity, config)\n",
    "        all_results[dataset_name][tag] = result\n",
    "\n",
    "# ---------- save results to disk ----------\n",
    "os.makedirs(config['results_dir'], exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = os.path.join(config['results_dir'], f\"synflow_resnet20_{timestamp}.json\")\n",
    "\n",
    "serialisable = {}\n",
    "for ds_name, ds_results in all_results.items():\n",
    "    serialisable[ds_name] = {}\n",
    "    for tag, res in ds_results.items():\n",
    "        serialisable[ds_name][tag] = {\n",
    "            'sparsity': res['sparsity'],\n",
    "            'best_test_acc': res['best_test_acc'],\n",
    "            'final_test_acc': res['final_test_acc'],\n",
    "            'test_accs': res['history']['test_accs'],\n",
    "            'train_accs': res['history']['train_accs'],\n",
    "            'train_losses': res['history']['train_losses'],\n",
    "            'layer_sparsity': {k: v for k, v in res.get('layer_sparsity', {}).items()\n",
    "                               if k != 'overall'},\n",
    "            'overall_sparsity': res.get('layer_sparsity', {}).get('overall', 0.0),\n",
    "            'pruning_time': res['pruning_time'],\n",
    "            'training_time': res['training_time'],\n",
    "            'total_time': res['total_time'],\n",
    "        }\n",
    "\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(serialisable, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {save_path}\")\n",
    "for ds_name in config['datasets']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY — {ds_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Sparsity':>12s}  {'Best Acc':>10s}  {'Final Acc':>10s}  {'Prune (s)':>10s}  {'Total (s)':>10s}\")\n",
    "    print(\"-\" * 60)\n",
    "    for tag, res in all_results[ds_name].items():\n",
    "        print(f\"{tag:>12s}  {res['best_test_acc']:>9.2f}%  {res['final_test_acc']:>9.2f}%\"\n",
    "              f\"  {res['pruning_time']:>9.1f}  {res['total_time']:>9.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df05bab",
   "metadata": {},
   "source": [
    "## 6 — Visualise results\n",
    "\n",
    "**Row 1** (CIFAR-10): test accuracy curves + accuracy-vs-sparsity.\n",
    "**Row 2** (CIFAR-100): same layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3135e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "colors = {\n",
    "    'dense': '#2E86AB',\n",
    "    '30%':   '#A23B72',\n",
    "    '60%':   '#F18F01',\n",
    "}\n",
    "\n",
    "for row, dataset_name in enumerate(config['datasets']):\n",
    "    ds_results = all_results[dataset_name]\n",
    "    ax_curve = axes[row, 0]\n",
    "    ax_bar   = axes[row, 1]\n",
    "\n",
    "    # ---- Left: test accuracy over epochs ----\n",
    "    for tag, res in ds_results.items():\n",
    "        epochs = range(1, len(res['history']['test_accs']) + 1)\n",
    "        ax_curve.plot(epochs, res['history']['test_accs'],\n",
    "                      label=f\"{tag} (best {res['best_test_acc']:.2f}%)\",\n",
    "                      color=colors.get(tag, 'gray'), linewidth=1.5)\n",
    "    ax_curve.set_xlabel('Epoch', fontsize=12)\n",
    "    ax_curve.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    ax_curve.set_title(f'ResNet-20 / {dataset_name.upper()} — Test Accuracy',\n",
    "                       fontsize=13, fontweight='bold')\n",
    "    ax_curve.legend(fontsize=10)\n",
    "    ax_curve.grid(True, alpha=0.3)\n",
    "\n",
    "    # ---- Right: best accuracy vs sparsity ----\n",
    "    sp_vals, acc_vals = [], []\n",
    "    for tag, res in ds_results.items():\n",
    "        sp_vals.append(res['sparsity'] * 100)\n",
    "        acc_vals.append(res['best_test_acc'])\n",
    "    ax_bar.plot(sp_vals, acc_vals, 'o-', linewidth=2, markersize=8, color='#2E86AB')\n",
    "    for sp, acc in zip(sp_vals, acc_vals):\n",
    "        ax_bar.annotate(f'{acc:.2f}%', (sp, acc), textcoords='offset points',\n",
    "                        xytext=(0, 10), ha='center', fontsize=9,\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', fc='lightyellow', alpha=0.8))\n",
    "    ax_bar.set_xlabel('Sparsity (%)', fontsize=12)\n",
    "    ax_bar.set_ylabel('Best Test Accuracy (%)', fontsize=12)\n",
    "    ax_bar.set_title(f'SynFlow: Accuracy vs Sparsity ({dataset_name.upper()})',\n",
    "                     fontsize=13, fontweight='bold')\n",
    "    ax_bar.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['results_dir'], 'synflow_resnet20.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final table\n",
    "for ds_name in config['datasets']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL RESULTS — {ds_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Sparsity':>12s}  {'Best Acc':>10s}  {'Final Acc':>10s}  {'Prune (s)':>10s}  {'Total (s)':>10s}\")\n",
    "    print(\"-\" * 60)\n",
    "    for tag, res in all_results[ds_name].items():\n",
    "        print(f\"{tag:>12s}  {res['best_test_acc']:>9.2f}%  {res['final_test_acc']:>9.2f}%\"\n",
    "              f\"  {res['pruning_time']:>9.1f}  {res['total_time']:>9.1f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
