{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d431b5",
   "metadata": {},
   "source": [
    "# SynFlow Sanity-Check — VGG-16-BN on CIFAR-100\n",
    "\n",
    "Reproducing the pruning-at-init comparison from *Pruning neural networks without any data by iteratively conserving synaptic flow* (Tanaka et al., NeurIPS 2020), **Appendix 13.3 / Figure 3**.\n",
    "\n",
    "| Setting | Value |\n",
    "|---|---|\n",
    "| **Architecture** | VGG-16-BN (CIFAR variant, batch normalization) |\n",
    "| **Dataset** | CIFAR-100 |\n",
    "| **Initialization** | Kaiming Normal (weights); biases = 0 |\n",
    "| **Prunable params** | All Conv2d + Linear weights (no biases, no BN) |\n",
    "| **Pruning methods** | SynFlow, Random, Magnitude, SNIP, GraSP |\n",
    "| **SynFlow iterations** | 100, exponential schedule ρ^{−k/n} |\n",
    "| **Compression ratios ρ** | 10^α,  α ∈ {1, 1.5, 2} (sanity subset) |\n",
    "| **Optimizer** | SGD, momentum 0.9, weight decay 1e-4 |\n",
    "| **LR schedule** | 0.1 → ×0.1 at epochs 60, 120 |\n",
    "\n",
    "> **Sanity-check mode**: epochs = **10**, single seed, 7 ρ values.  \n",
    "> Increase `EPOCHS` and add seeds for the full reproduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969681e",
   "metadata": {},
   "source": [
    "## 1 — Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, copy, json, time, math\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Project imports\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from model import vgg16, count_parameters\n",
    "from synflow import synflow_pruning, apply_synflow_masks, get_synflow_sparsity\n",
    "from train import train_epochs, evaluate\n",
    "from util import apply_masks_to_model, create_mask_apply_fn, set_seed\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8904f",
   "metadata": {},
   "source": [
    "## 2 — Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b1b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sanity-check config ──────────────────────────────────────────\n",
    "# Increase EPOCHS to 160 and SEEDS to [42, 43, 44] for the full run.\n",
    "\n",
    "EPOCHS       = 160          # 160 for full reproduction\n",
    "SEEDS        = [42]        # [42, 43, 44] for 3 runs\n",
    "BATCH_SIZE   = 128\n",
    "LR           = 0.1\n",
    "MOMENTUM     = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LR_MILESTONES = [60, 120]\n",
    "LR_GAMMA     = 0.1\n",
    "\n",
    "# Compression ratios (paper sweeps α ∈ [0, 0.25, …, 4] → ρ = 10^α)\n",
    "# Sanity-check subset: 7 points\n",
    "ALPHAS = [0.0, 1.0, 1.5, 2.0]\n",
    "RHOS   = [10**a for a in ALPHAS]\n",
    "\n",
    "# Methods to compare\n",
    "METHODS = ['synflow', 'random', 'magnitude', 'snip', 'grasp']\n",
    "\n",
    "# SNIP / GraSP data budget\n",
    "SNIP_GRASP_SAMPLES = 1000   # total samples from CIFAR-100\n",
    "\n",
    "INPUT_SHAPE = (3, 32, 32)\n",
    "NUM_CLASSES = 100\n",
    "SYNFLOW_ITERS = 100\n",
    "RESULTS_DIR = '../results/synflow_sanity'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SANITY-CHECK CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model          : VGG-16-BN\")\n",
    "print(f\"  Dataset        : CIFAR-100\")\n",
    "print(f\"  Epochs         : {EPOCHS}\")\n",
    "print(f\"  Seeds          : {SEEDS}\")\n",
    "print(f\"  Methods        : {METHODS}\")\n",
    "print(f\"  ρ values       : {[f'{r:.2f}' for r in RHOS]}\")\n",
    "print(f\"  α values       : {ALPHAS}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d2a7aa",
   "metadata": {},
   "source": [
    "## 3 — Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e3493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.5071, 0.4867, 0.4408)\n",
    "std  = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR100(root='../data', train=True,\n",
    "                                          download=True, transform=train_transform)\n",
    "test_ds  = torchvision.datasets.CIFAR100(root='../data', train=False,\n",
    "                                          download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=256,\n",
    "                          shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"CIFAR-100  Train: {len(train_ds):,}  Test: {len(test_ds):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483de0ec",
   "metadata": {},
   "source": [
    "## 4 — Baseline scoring functions\n",
    "\n",
    "Five pruning-at-init strategies, all returning `Dict[str, np.ndarray]` masks:\n",
    "\n",
    "| Method | Score | Mode | Data-free? |\n",
    "|---|---|---|---|\n",
    "| **SynFlow** | ∂R/∂θ ⊙ θ  (iterative, 100 rounds) | eval | ✓ |\n",
    "| **Random** | Gaussian i.i.d. | — | ✓ |\n",
    "| **Magnitude** | \\|θ\\| | — | ✓ |\n",
    "| **SNIP** | \\|∂L/∂θ ⊙ θ\\| on one mini-batch | train | ✗ |\n",
    "| **GraSP** | −(H ∂L/∂θ) ⊙ θ | train | ✗ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Helper: create a fresh VGG-16-BN with Kaiming init ──────────\n",
    "\n",
    "def make_model(seed):\n",
    "    \"\"\"Return a freshly initialised VGG-16-BN for CIFAR-100.\"\"\"\n",
    "    set_seed(seed)\n",
    "    model = vgg16(num_classes=NUM_CLASSES, batch_norm=True).to(device)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ── Generic one-shot masking from per-param scores ──────────────\n",
    "\n",
    "def _scores_to_masks(scores: dict, rho: float) -> dict:\n",
    "    \"\"\"Global threshold: keep top 1/ρ fraction of scores (by magnitude).\"\"\"\n",
    "    keep_ratio = 1.0 / rho\n",
    "    all_s = torch.cat([s.view(-1) for s in scores.values()])\n",
    "    n = all_s.numel()\n",
    "    k = max(int(keep_ratio * n), 1)\n",
    "    if k < n:\n",
    "        thr, _ = torch.kthvalue(all_s.view(-1), n - k + 1)\n",
    "    else:\n",
    "        thr = torch.tensor(-float('inf'))\n",
    "    masks = {}\n",
    "    for name, s in scores.items():\n",
    "        masks[name] = (s >= thr).float().numpy()\n",
    "    return masks\n",
    "\n",
    "\n",
    "# ── Random pruning ──────────────────────────────────────────────\n",
    "\n",
    "def random_pruning(model, rho, seed=0):\n",
    "    \"\"\"Score = i.i.d. Gaussian → global top-1/ρ mask.\"\"\"\n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "    scores = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            scores[name] = torch.randn(module.weight.shape, generator=rng).abs()\n",
    "    return _scores_to_masks(scores, rho)\n",
    "\n",
    "\n",
    "# ── Magnitude pruning ──────────────────────────────────────────\n",
    "\n",
    "def magnitude_pruning(model, rho):\n",
    "    \"\"\"Score = |θ| → global top-1/ρ mask.\"\"\"\n",
    "    scores = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            scores[name] = module.weight.data.detach().abs().cpu()\n",
    "    return _scores_to_masks(scores, rho)\n",
    "\n",
    "\n",
    "# ── SNIP  (|∂L/∂θ ⊙ θ|, single forward-backward in train mode) ─\n",
    "\n",
    "def snip_pruning(model, rho, dataloader, num_samples=1000):\n",
    "    \"\"\"SNIP: connection sensitivity |∂L/∂(θ⊙1)| ≈ |g⊙θ|.\"\"\"\n",
    "    net = copy.deepcopy(model).to(device)\n",
    "    net.train()                         # paper: train mode (Appendix 13.1.1)\n",
    "    net.zero_grad()\n",
    "\n",
    "    # Collect a single large batch\n",
    "    imgs, lbls = [], []\n",
    "    for x, y in dataloader:\n",
    "        imgs.append(x); lbls.append(y)\n",
    "        if sum(i.size(0) for i in imgs) >= num_samples:\n",
    "            break\n",
    "    imgs = torch.cat(imgs)[:num_samples].to(device)\n",
    "    lbls = torch.cat(lbls)[:num_samples].to(device)\n",
    "\n",
    "    out = net(imgs)\n",
    "    loss = F.cross_entropy(out, lbls)\n",
    "    loss.backward()\n",
    "\n",
    "    scores = {}\n",
    "    for name, module in net.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            if module.weight.grad is not None:\n",
    "                scores[name] = (module.weight.grad * module.weight.data).detach().abs().cpu()\n",
    "    return _scores_to_masks(scores, rho)\n",
    "\n",
    "\n",
    "# ── GraSP  (−Hg ⊙ θ, in train mode) ───────────────────────────\n",
    "\n",
    "def grasp_pruning(model, rho, dataloader, num_samples=1000, T=200.0):\n",
    "    \"\"\"GraSP: gradient signal preservation score −(Hg)⊙θ.\"\"\"\n",
    "    net = copy.deepcopy(model).to(device)\n",
    "    net.train()                         # train mode (Appendix 13.1.1)\n",
    "    net.zero_grad()\n",
    "\n",
    "    weights = [m.weight for m in net.modules()\n",
    "               if isinstance(m, (nn.Conv2d, nn.Linear))]\n",
    "    for w in weights:\n",
    "        w.requires_grad_(True)\n",
    "\n",
    "    # Collect data\n",
    "    imgs, lbls = [], []\n",
    "    for x, y in dataloader:\n",
    "        imgs.append(x); lbls.append(y)\n",
    "        if sum(i.size(0) for i in imgs) >= num_samples:\n",
    "            break\n",
    "    imgs = torch.cat(imgs)[:num_samples].to(device)\n",
    "    lbls = torch.cat(lbls)[:num_samples].to(device)\n",
    "    N = imgs.size(0)\n",
    "\n",
    "    # ---- first-order gradient g (no graph) ----\n",
    "    out1 = net(imgs[:N//2]) / T\n",
    "    loss1 = F.cross_entropy(out1, lbls[:N//2])\n",
    "    g = list(autograd.grad(loss1, weights))\n",
    "\n",
    "    out2 = net(imgs[N//2:]) / T\n",
    "    loss2 = F.cross_entropy(out2, lbls[N//2:])\n",
    "    g2 = autograd.grad(loss2, weights, create_graph=False)\n",
    "    g = [gi + g2i for gi, g2i in zip(g, g2)]\n",
    "\n",
    "    # ---- Hg via z = gᵀ g_f ----\n",
    "    out_f = net(imgs[:N//2]) / T\n",
    "    loss_f = F.cross_entropy(out_f, lbls[:N//2])\n",
    "    g_f = autograd.grad(loss_f, weights, create_graph=True)\n",
    "    z = sum((gi.data * gfi).sum() for gi, gfi in zip(g, g_f))\n",
    "    z.backward()\n",
    "\n",
    "    out_f2 = net(imgs[N//2:]) / T\n",
    "    loss_f2 = F.cross_entropy(out_f2, lbls[N//2:])\n",
    "    g_f2 = autograd.grad(loss_f2, weights, create_graph=True)\n",
    "    z2 = sum((gi.data * gfi).sum() for gi, gfi in zip(g, g_f2))\n",
    "    z2.backward()\n",
    "\n",
    "    # Score = −θ ⊙ Hg  (higher = more important)\n",
    "    scores = {}\n",
    "    for name, module in net.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            if module.weight.grad is not None:\n",
    "                scores[name] = (-module.weight.data * module.weight.grad).detach().cpu()\n",
    "    return _scores_to_masks(scores, rho)\n",
    "\n",
    "\n",
    "print(\"Scoring functions defined: random, magnitude, snip, grasp, synflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bedf95",
   "metadata": {},
   "source": [
    "## 5 — Run sanity-check sweep\n",
    "\n",
    "For each method × ρ × seed: prune → train → record test accuracy.  \n",
    "Dense baseline (ρ = 1) is shared across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pruned(model, masks, seed):\n",
    "    \"\"\"Train a pruned (or dense) model and return best / final test acc.\"\"\"\n",
    "    set_seed(seed)\n",
    "    apply_fn = create_mask_apply_fn(model) if masks is not None else None\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR,\n",
    "                          momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=LR_MILESTONES, gamma=LR_GAMMA)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    t0 = time.time()\n",
    "    history = train_epochs(\n",
    "        model=model, train_loader=train_loader, test_loader=test_loader,\n",
    "        criterion=criterion, optimizer=optimizer, num_epochs=EPOCHS,\n",
    "        device=device, scheduler=scheduler, masks=masks,\n",
    "        apply_mask_fn=apply_fn, verbose=False,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    best = max(history['test_accs'])\n",
    "    final = history['final_test_acc']\n",
    "    return best, final, elapsed, history\n",
    "\n",
    "\n",
    "# ── Main loop ───────────────────────────────────────────────────\n",
    "# results[method][rho] = list of (best_acc, final_acc) over seeds\n",
    "\n",
    "results = {m: {} for m in METHODS}\n",
    "all_histories = {m: {} for m in METHODS}  # for plotting curves\n",
    "\n",
    "total_runs = len(METHODS) * len(RHOS) * len(SEEDS)\n",
    "run_idx = 0\n",
    "\n",
    "for seed in SEEDS:\n",
    "    for rho in RHOS:\n",
    "        alpha = math.log10(rho)\n",
    "        sparsity = 1 - 1/rho if rho > 1 else 0.0\n",
    "\n",
    "        for method in METHODS:\n",
    "            run_idx += 1\n",
    "            tag = f\"[{run_idx}/{total_runs}] {method:>10s}  ρ={rho:>8.2f} (α={alpha:.2f}, s={sparsity:.1%})  seed={seed}\"\n",
    "            print(f\"\\n{'─'*70}\\n{tag}\")\n",
    "\n",
    "            model = make_model(seed)\n",
    "\n",
    "            # ── Pruning ──\n",
    "            t_prune = time.time()\n",
    "            masks = None\n",
    "            if rho == 1.0:\n",
    "                # Dense baseline — no pruning\n",
    "                pass\n",
    "            elif method == 'synflow':\n",
    "                masks = synflow_pruning(model, device, rho=rho,\n",
    "                                        num_iters=SYNFLOW_ITERS,\n",
    "                                        input_shape=INPUT_SHAPE)\n",
    "                apply_synflow_masks(model, masks)\n",
    "            elif method == 'random':\n",
    "                masks = random_pruning(model, rho, seed=seed)\n",
    "                apply_synflow_masks(model, masks)\n",
    "            elif method == 'magnitude':\n",
    "                masks = magnitude_pruning(model, rho)\n",
    "                apply_synflow_masks(model, masks)\n",
    "            elif method == 'snip':\n",
    "                masks = snip_pruning(model, rho, train_loader,\n",
    "                                     num_samples=SNIP_GRASP_SAMPLES)\n",
    "                # snip_pruning returns masks but doesn't apply them\n",
    "                apply_synflow_masks(model, masks)\n",
    "            elif method == 'grasp':\n",
    "                masks = grasp_pruning(model, rho, train_loader,\n",
    "                                      num_samples=SNIP_GRASP_SAMPLES)\n",
    "                apply_synflow_masks(model, masks)\n",
    "            prune_time = time.time() - t_prune\n",
    "\n",
    "            if masks is not None:\n",
    "                sp = get_synflow_sparsity(masks)\n",
    "                print(f\"  Pruned in {prune_time:.1f}s — actual sparsity {sp['overall']:.2%}\")\n",
    "            else:\n",
    "                print(f\"  Dense (no pruning)\")\n",
    "\n",
    "            # ── Train ──\n",
    "            best, final, train_time, hist = train_pruned(model, masks, seed)\n",
    "            print(f\"  Train {EPOCHS} ep in {train_time:.1f}s — best {best:.2f}%, final {final:.2f}%\")\n",
    "\n",
    "            results[method].setdefault(rho, []).append((best, final))\n",
    "            all_histories[method].setdefault(rho, []).append(hist)\n",
    "\n",
    "print(f\"\\n{'='*70}\\nAll {total_runs} runs complete.\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df05bab",
   "metadata": {},
   "source": [
    "## 6 — Visualise: Accuracy vs Compression Ratio ρ\n",
    "\n",
    "Reproduces the paper's Figure 3 style — log-scale x-axis (ρ = 10^α), one line per method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3135e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_styles = {\n",
    "    'synflow':   {'color': '#E63946', 'marker': 'o', 'label': 'SynFlow'},\n",
    "    'random':    {'color': '#A8DADC', 'marker': 's', 'label': 'Random'},\n",
    "    'magnitude': {'color': '#457B9D', 'marker': '^', 'label': 'Magnitude'},\n",
    "    'snip':      {'color': '#F4A261', 'marker': 'D', 'label': 'SNIP'},\n",
    "    'grasp':     {'color': '#2A9D8F', 'marker': 'v', 'label': 'GraSP'},\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "for method in METHODS:\n",
    "    rho_vals = sorted(results[method].keys())\n",
    "    mean_accs, min_accs, max_accs = [], [], []\n",
    "    for r in rho_vals:\n",
    "        bests = [b for b, _ in results[method][r]]\n",
    "        mean_accs.append(np.mean(bests))\n",
    "        min_accs.append(np.min(bests))\n",
    "        max_accs.append(np.max(bests))\n",
    "\n",
    "    sty = method_styles[method]\n",
    "    ax.plot(rho_vals, mean_accs, marker=sty['marker'], color=sty['color'],\n",
    "            label=sty['label'], linewidth=2, markersize=7)\n",
    "    if len(SEEDS) > 1:\n",
    "        ax.fill_between(rho_vals, min_accs, max_accs,\n",
    "                        color=sty['color'], alpha=0.15)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Compression Ratio  ρ', fontsize=13)\n",
    "ax.set_ylabel('Best Test Accuracy (%)', fontsize=13)\n",
    "ax.set_title('VGG-16-BN / CIFAR-100 — Pruning-at-Init Comparison\\n'\n",
    "             f'(sanity check, {EPOCHS} epochs, seed={SEEDS})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='lower left')\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Secondary x-axis showing α = log10(ρ)\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "alpha_ticks = [10**a for a in ALPHAS]\n",
    "ax2.set_xticks(alpha_ticks)\n",
    "ax2.set_xticklabels([f'{a:.1f}' for a in ALPHAS])\n",
    "ax2.set_xlabel('α  (ρ = 10^α)', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'sanity_check_vgg16_cifar100.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ── Summary table ───────────────────────────────────────────────\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"SUMMARY — VGG-16-BN / CIFAR-100  ({EPOCHS} epochs)\")\n",
    "print(f\"{'='*90}\")\n",
    "header = f\"{'ρ':>10s}  {'α':>5s}  {'Sparsity':>10s}\"\n",
    "for m in METHODS:\n",
    "    header += f\"  {method_styles[m]['label']:>10s}\"\n",
    "print(header)\n",
    "print(\"-\" * 90)\n",
    "for rho in sorted(RHOS):\n",
    "    alpha = math.log10(rho) if rho > 0 else 0\n",
    "    sp = 1 - 1/rho if rho > 1 else 0\n",
    "    row = f\"{rho:>10.2f}  {alpha:>5.2f}  {sp:>9.1%}\"\n",
    "    for m in METHODS:\n",
    "        bests = [b for b, _ in results[m].get(rho, [(0, 0)])]\n",
    "        row += f\"  {np.mean(bests):>9.2f}%\"\n",
    "    print(row)\n",
    "\n",
    "# ── Save JSON ───────────────────────────────────────────────────\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = os.path.join(RESULTS_DIR, f\"sanity_{timestamp}.json\")\n",
    "serialisable = {}\n",
    "for m in METHODS:\n",
    "    serialisable[m] = {}\n",
    "    for rho, vals in results[m].items():\n",
    "        serialisable[m][str(rho)] = {\n",
    "            'rho': rho,\n",
    "            'alpha': math.log10(rho) if rho > 0 else 0,\n",
    "            'sparsity': 1 - 1/rho if rho > 1 else 0,\n",
    "            'best_accs': [b for b, _ in vals],\n",
    "            'final_accs': [f for _, f in vals],\n",
    "        }\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(serialisable, f, indent=2)\n",
    "print(f\"\\nResults saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
