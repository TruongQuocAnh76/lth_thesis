{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb6c332",
   "metadata": {},
   "source": [
    "# Early-Bird Ticket Discovery on VGG16 + CIFAR-10\n",
    "\n",
    "This notebook implements the Early-Bird (EB) ticket discovery algorithm from \"Drawing Early-Bird Tickets\" (You et al., ICLR 2020).\n",
    "\n",
    "## Configuration\n",
    "- **Model**: VGG16 with Batch Normalization\n",
    "- **Dataset**: CIFAR-10\n",
    "- **Optimizer**: SGD with momentum (0.9) and weight decay (10‚Åª‚Å¥)\n",
    "- **Learning Rate**: 0.1, divided by 10 at epochs 80 and 120\n",
    "- **Total Epochs**: 160\n",
    "- **Batch Size**: 256\n",
    "- **Pruning**: Structured channel pruning based on BN Œ≥ values\n",
    "- **Exit Criterion**: Mask Hamming distance < Œµ (0.1) for 5 consecutive epochs\n",
    "- **Target Sparsities**: 30%, 50%, 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59fc293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minad/lth_thesis/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "ROOT_DIR = Path.cwd().parent\n",
    "if str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import our Early-Bird implementation\n",
    "from src.earlybird import (\n",
    "    EarlyBirdFinder,\n",
    "    extract_bn_gammas,\n",
    "    compute_channel_mask_from_bn_gamma,\n",
    "    expand_channel_mask_to_conv_weights,\n",
    "    add_l1_regularization_to_bn,\n",
    "    get_bn_layer_count,\n",
    "    get_overall_channel_sparsity,\n",
    "    get_channel_sparsity,\n",
    ")\n",
    "from src.data import get_cifar10_dataloaders\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986d3f6",
   "metadata": {},
   "source": [
    "## VGG16 Model Definition\n",
    "\n",
    "VGG16 with Batch Normalization for CIFAR-10. The model follows the standard VGG architecture adapted for 32√ó32 input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73df3eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16-BN created with 13 BatchNorm layers\n",
      "Total parameters: 15,253,578\n"
     ]
    }
   ],
   "source": [
    "class VGG16_BN(nn.Module):\n",
    "    \"\"\"VGG16 with Batch Normalization for CIFAR-10.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Configuration for VGG16: number of channels per block\n",
    "        # 2 conv layers with 64 -> 2 with 128 -> 3 with 256 -> 3 with 512 -> 3 with 512\n",
    "        cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "        \n",
    "        self.features = self._make_layers(cfg)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels, x, kernel_size=3, padding=1))\n",
    "                layers.append(nn.BatchNorm2d(x))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create model and check BN layer count\n",
    "model = VGG16_BN(num_classes=10).to(device)\n",
    "bn_count = get_bn_layer_count(model)\n",
    "print(f\"VGG16-BN created with {bn_count} BatchNorm layers\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a9ed8",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load CIFAR-10 with standard data augmentation (random crop, horizontal flip) and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0990250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [01:54<00:00, 1.48MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50,000\n",
      "Test samples: 10,000\n",
      "Batch size: 256\n",
      "Training batches per epoch: 196\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Get CIFAR-10 dataloaders\n",
    "train_loader, test_loader = get_cifar10_dataloaders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    data_dir=ROOT_DIR / 'data'\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_loader.dataset):,}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset):,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd28d2",
   "metadata": {},
   "source": [
    "## Early-Bird Search Configuration\n",
    "\n",
    "Setup the training and Early-Bird detection parameters according to the paper specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac96119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Early-Bird Search Configuration\n",
      "============================================================\n",
      "Target sparsity: 50%\n",
      "Distance threshold (Œµ): 0.1\n",
      "Patience: 5 consecutive epochs\n",
      "L1 coefficient: 0.0001\n",
      "Pruning method: global\n",
      "============================================================\n",
      "Training Configuration\n",
      "============================================================\n",
      "Total epochs: 160\n",
      "Initial LR: 0.1\n",
      "LR milestones: [80, 120]\n",
      "Momentum: 0.9\n",
      "Weight decay: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "TOTAL_EPOCHS = 160\n",
    "INITIAL_LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LR_MILESTONES = [80, 120]  # Divide LR by 10 at these epochs\n",
    "LR_GAMMA = 0.1\n",
    "\n",
    "# Early-Bird detection configuration\n",
    "TARGET_SPARSITY = 0.5  # 50% pruning ratio (change to 0.3, 0.5, or 0.7)\n",
    "DISTANCE_THRESHOLD = 0.1  # Œµ: Hamming distance threshold\n",
    "PATIENCE = 5  # Number of consecutive epochs below threshold\n",
    "L1_COEF = 1e-4  # L1 regularization coefficient for BN Œ≥\n",
    "PRUNING_METHOD = 'global'  # 'global' or 'layerwise'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Early-Bird Search Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Target sparsity: {TARGET_SPARSITY * 100:.0f}%\")\n",
    "print(f\"Distance threshold (Œµ): {DISTANCE_THRESHOLD}\")\n",
    "print(f\"Patience: {PATIENCE} consecutive epochs\")\n",
    "print(f\"L1 coefficient: {L1_COEF}\")\n",
    "print(f\"Pruning method: {PRUNING_METHOD}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total epochs: {TOTAL_EPOCHS}\")\n",
    "print(f\"Initial LR: {INITIAL_LR}\")\n",
    "print(f\"LR milestones: {LR_MILESTONES}\")\n",
    "print(f\"Momentum: {MOMENTUM}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32079dcf",
   "metadata": {},
   "source": [
    "## Training Functions\n",
    "\n",
    "Define the training loop with L1 regularization on BatchNorm Œ≥ values for the search phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d5534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined.\n"
     ]
    }
   ],
   "source": [
    "def train_epoch_with_l1(model, train_loader, optimizer, criterion, l1_coef, device):\n",
    "    \"\"\"Train for one epoch with L1 regularization on BN Œ≥ values.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_l1_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        ce_loss = criterion(outputs, targets)\n",
    "        \n",
    "        # L1 regularization on BN Œ≥\n",
    "        l1_loss = add_l1_regularization_to_bn(model, l1_coef)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = ce_loss + l1_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += ce_loss.item() * inputs.size(0)\n",
    "        total_l1_loss += l1_loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    avg_l1_loss = total_l1_loss / total\n",
    "    accuracy = 100.0 * correct / total\n",
    "    \n",
    "    return avg_loss, avg_l1_loss, accuracy\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100.0 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd561ab",
   "metadata": {},
   "source": [
    "## Phase 1: Early-Bird Ticket Search\n",
    "\n",
    "Train the network with L1 regularization on BN Œ≥ values while monitoring mask stability. The search terminates when the Hamming distance between consecutive epoch masks stays below Œµ for `patience` consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd8de93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Early-Bird ticket search...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize model for fresh start\n",
    "model = VGG16_BN(num_classes=10).to(device)\n",
    "\n",
    "# Setup optimizer, scheduler, and criterion\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=INITIAL_LR,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = MultiStepLR(optimizer, milestones=LR_MILESTONES, gamma=LR_GAMMA)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize Early-Bird finder\n",
    "eb_finder = EarlyBirdFinder(\n",
    "    target_sparsity=TARGET_SPARSITY,\n",
    "    patience=PATIENCE,\n",
    "    distance_threshold=DISTANCE_THRESHOLD,\n",
    "    pruning_method=PRUNING_METHOD\n",
    ")\n",
    "\n",
    "# Tracking for visualization\n",
    "history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'mask_distance': [],\n",
    "    'lr': [],\n",
    "}\n",
    "\n",
    "print(\"Starting Early-Bird ticket search...\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d528767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minad/lth_thesis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m current_lr = optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Train one epoch with L1 regularization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m train_loss, l1_loss, train_acc = \u001b[43mtrain_epoch_with_l1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL1_COEF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m     13\u001b[39m test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_epoch_with_l1\u001b[39m\u001b[34m(model, train_loader, optimizer, criterion, l1_coef, device)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Combined loss\u001b[39;00m\n\u001b[32m     22\u001b[39m loss = ce_loss + l1_loss\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m optimizer.step()\n\u001b[32m     27\u001b[39m total_loss += ce_loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lth_thesis/.venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lth_thesis/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lth_thesis/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Main training loop for Early-Bird search\n",
    "convergence_epoch = None\n",
    "\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Train one epoch with L1 regularization\n",
    "    train_loss, l1_loss, train_acc = train_epoch_with_l1(\n",
    "        model, train_loader, optimizer, criterion, L1_COEF, device\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Record BN Œ≥ and check for Early-Bird convergence\n",
    "    converged, mask_distance = eb_finder.record_epoch(model, epoch)\n",
    "    \n",
    "    # Store history\n",
    "    history['epoch'].append(epoch)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['mask_distance'].append(mask_distance if mask_distance is not None else -1)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print progress (every epoch for first 10, then every 5)\n",
    "    if epoch < 10 or epoch % 5 == 0 or converged:\n",
    "        dist_str = f\"{mask_distance:.4f}\" if mask_distance is not None else \"N/A\"\n",
    "        print(f\"Epoch {epoch:3d} | LR: {current_lr:.4f} | \"\n",
    "              f\"Train: {train_acc:.2f}% (Loss: {train_loss:.4f}) | \"\n",
    "              f\"Test: {test_acc:.2f}% (Loss: {test_loss:.4f}) | \"\n",
    "              f\"Mask Dist: {dist_str}\" + \n",
    "              (\" ‚Üê EB Ticket!\" if converged else \"\"))\n",
    "    \n",
    "    # Check for convergence\n",
    "    if converged:\n",
    "        convergence_epoch = epoch\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"üé´ EARLY-BIRD TICKET FOUND at epoch {epoch}!\")\n",
    "        print(\"=\" * 70)\n",
    "        break\n",
    "\n",
    "if convergence_epoch is None:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚ö†Ô∏è  Early-Bird ticket not found within training epochs.\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Consider: lower distance threshold, higher patience, or more epochs.\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Search phase complete. Early-Bird ticket emerged at epoch {convergence_epoch}.\")\n",
    "    print(f\"‚úì Training epochs saved: {TOTAL_EPOCHS - convergence_epoch - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29a10e",
   "metadata": {},
   "source": [
    "## Early-Bird Results Analysis\n",
    "\n",
    "Examine the discovered Early-Bird ticket and visualize the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Early-Bird statistics\n",
    "eb_stats = eb_finder.get_statistics()\n",
    "eb_ticket = eb_finder.get_early_bird_ticket()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EARLY-BIRD SEARCH STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Converged: {eb_stats['converged']}\")\n",
    "print(f\"Convergence Epoch: {eb_stats['convergence_epoch']}\")\n",
    "print(f\"Total Epochs Recorded: {eb_stats['total_epochs_recorded']}\")\n",
    "print(f\"Target Sparsity: {eb_stats['target_sparsity'] * 100:.1f}%\")\n",
    "print(f\"Distance Threshold (Œµ): {eb_stats['distance_threshold']}\")\n",
    "print(f\"Patience: {eb_stats['patience']}\")\n",
    "\n",
    "if eb_ticket is not None:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CHANNEL MASK STATISTICS (per BN layer)\")\n",
    "    print(\"=\" * 70)\n",
    "    sparsity_per_layer = get_channel_sparsity(eb_ticket)\n",
    "    \n",
    "    # Print first 5 and last 5 layers\n",
    "    items = list(sparsity_per_layer.items())\n",
    "    if len(items) <= 10:\n",
    "        for name, sparsity in items:\n",
    "            remaining = (1 - sparsity) * 100\n",
    "            print(f\"  {name}: {sparsity * 100:.2f}% pruned ({remaining:.2f}% remaining)\")\n",
    "    else:\n",
    "        for name, sparsity in items[:5]:\n",
    "            remaining = (1 - sparsity) * 100\n",
    "            print(f\"  {name}: {sparsity * 100:.2f}% pruned ({remaining:.2f}% remaining)\")\n",
    "        print(f\"  ... ({len(items) - 10} layers omitted) ...\")\n",
    "        for name, sparsity in items[-5:]:\n",
    "            remaining = (1 - sparsity) * 100\n",
    "            print(f\"  {name}: {sparsity * 100:.2f}% pruned ({remaining:.2f}% remaining)\")\n",
    "    \n",
    "    overall_sparsity = get_overall_channel_sparsity(eb_ticket)\n",
    "    print(f\"\\n  Overall Channel Sparsity: {overall_sparsity * 100:.2f}%\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366323cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history and mask distance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training and Test Accuracy\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['epoch'], history['train_acc'], label='Train Acc', color='#2E86AB', linewidth=2)\n",
    "ax1.plot(history['epoch'], history['test_acc'], label='Test Acc', color='#F18F01', linewidth=2)\n",
    "if convergence_epoch is not None:\n",
    "    ax1.axvline(x=convergence_epoch, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'EB Ticket (epoch {convergence_epoch})')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Training Progress', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Loss\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['epoch'], history['train_loss'], label='Train Loss', color='#2E86AB', linewidth=2)\n",
    "ax2.plot(history['epoch'], history['test_loss'], label='Test Loss', color='#F18F01', linewidth=2)\n",
    "if convergence_epoch is not None:\n",
    "    ax2.axvline(x=convergence_epoch, color='red', linestyle='--', linewidth=2, label=f'EB Ticket')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.set_title('Loss Curves', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Mask Distance over epochs\n",
    "ax3 = axes[1, 0]\n",
    "distances = history['mask_distance']\n",
    "valid_distances = [(e, d) for e, d in zip(history['epoch'], distances) if d >= 0]\n",
    "if valid_distances:\n",
    "    epochs_valid, dists_valid = zip(*valid_distances)\n",
    "    ax3.plot(epochs_valid, dists_valid, 'o-', color='#2E86AB', markersize=4, linewidth=2, label='Mask Distance')\n",
    "    ax3.axhline(y=DISTANCE_THRESHOLD, color='#A23B72', linestyle='--', linewidth=2,\n",
    "                label=f'Threshold (Œµ={DISTANCE_THRESHOLD})')\n",
    "    if convergence_epoch is not None:\n",
    "        ax3.axvline(x=convergence_epoch, color='red', linestyle='--', linewidth=2, label=f'EB Ticket')\n",
    "ax3.set_xlabel('Epoch', fontsize=12)\n",
    "ax3.set_ylabel('Hamming Distance', fontsize=12)\n",
    "ax3.set_title('Mask Stability (Hamming Distance)', fontsize=14, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Learning Rate Schedule\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history['epoch'], history['lr'], color='#A23B72', linewidth=2)\n",
    "ax4.set_xlabel('Epoch', fontsize=12)\n",
    "ax4.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax4.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT_DIR / 'results' / 'earlybird_vgg16_cifar10_search.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Figure saved to results/earlybird_vgg16_cifar10_search.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126fec29",
   "metadata": {},
   "source": [
    "## Phase 2: Fine-tune the Pruned Network\n",
    "\n",
    "After discovering the Early-Bird ticket, we apply the channel mask to prune the network and fine-tune to recover accuracy. The fine-tuning phase trains **without** L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_channel_mask(model, channel_masks, weight_masks):\n",
    "    \"\"\"\n",
    "    Apply channel pruning by zeroing out pruned channels in Conv2d and BN layers.\n",
    "    In practice, you would reconstruct a smaller network, but for simplicity,\n",
    "    we zero out the weights corresponding to pruned channels.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Apply to Conv2d layers\n",
    "        for name, module in model.named_modules():\n",
    "            if name in weight_masks:\n",
    "                mask_tensor = torch.from_numpy(weight_masks[name]).float().to(module.weight.device)\n",
    "                module.weight.data *= mask_tensor\n",
    "                \n",
    "        # Zero out BN parameters for pruned channels\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.BatchNorm2d) and name in channel_masks:\n",
    "                mask = torch.from_numpy(channel_masks[name]).float().to(module.weight.device)\n",
    "                module.weight.data *= mask\n",
    "                module.bias.data *= mask\n",
    "                module.running_mean *= mask\n",
    "                module.running_var = module.running_var * mask + (1 - mask) * 1.0  # Avoid div by zero\n",
    "\n",
    "def train_epoch_standard(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch without L1 regularization.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / total, 100.0 * correct / total\n",
    "\n",
    "print(\"Fine-tuning functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb21e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eb_ticket is not None:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"APPLYING EARLY-BIRD TICKET\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Expand channel masks to Conv2d weight masks\n",
    "    weight_masks = expand_channel_mask_to_conv_weights(eb_ticket, model)\n",
    "    \n",
    "    print(f\"Channel masks (BN layers): {len(eb_ticket)}\")\n",
    "    print(f\"Weight masks (Conv layers): {len(weight_masks)}\")\n",
    "    \n",
    "    # Apply the mask (zero out pruned channels)\n",
    "    apply_channel_mask(model, eb_ticket, weight_masks)\n",
    "    print(\"‚úì Pruning masks applied to model weights\")\n",
    "    \n",
    "    # Verify sparsity after masking\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += (param == 0).sum().item()\n",
    "    \n",
    "    actual_sparsity = zero_params / total_params * 100\n",
    "    print(f\"Actual weight sparsity: {actual_sparsity:.2f}%\")\n",
    "    \n",
    "    # Evaluate immediately after pruning\n",
    "    test_loss_pruned, test_acc_pruned = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"\\nTest accuracy after pruning (before fine-tuning): {test_acc_pruned:.2f}%\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"No Early-Bird ticket found. Skipping fine-tuning.\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a56aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eb_ticket is not None:\n",
    "    # Fine-tuning configuration\n",
    "    FINETUNE_EPOCHS = TOTAL_EPOCHS - convergence_epoch  # Remaining epochs\n",
    "    \n",
    "    # Reset optimizer and scheduler for fine-tuning\n",
    "    # Start with reduced LR based on where we stopped\n",
    "    finetune_lr = INITIAL_LR\n",
    "    for milestone in LR_MILESTONES:\n",
    "        if convergence_epoch >= milestone:\n",
    "            finetune_lr *= LR_GAMMA\n",
    "    \n",
    "    optimizer_ft = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=finetune_lr,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Adjust milestones relative to convergence epoch\n",
    "    remaining_milestones = [m - convergence_epoch for m in LR_MILESTONES if m > convergence_epoch]\n",
    "    scheduler_ft = MultiStepLR(optimizer_ft, milestones=remaining_milestones, gamma=LR_GAMMA)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINE-TUNING PRUNED NETWORK\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Fine-tuning epochs: {FINETUNE_EPOCHS}\")\n",
    "    print(f\"Initial fine-tuning LR: {finetune_lr}\")\n",
    "    print(f\"Remaining LR milestones: {remaining_milestones}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    finetune_history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': [],\n",
    "    }\n",
    "    \n",
    "    best_test_acc = test_acc_pruned\n",
    "    \n",
    "    for ft_epoch in range(FINETUNE_EPOCHS):\n",
    "        global_epoch = convergence_epoch + ft_epoch + 1\n",
    "        current_lr = optimizer_ft.param_groups[0]['lr']\n",
    "        \n",
    "        # Train without L1 regularization\n",
    "        train_loss, train_acc = train_epoch_standard(\n",
    "            model, train_loader, optimizer_ft, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Re-apply mask after optimizer step (maintain sparsity)\n",
    "        apply_channel_mask(model, eb_ticket, weight_masks)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        scheduler_ft.step()\n",
    "        \n",
    "        # Track history\n",
    "        finetune_history['epoch'].append(global_epoch)\n",
    "        finetune_history['train_loss'].append(train_loss)\n",
    "        finetune_history['train_acc'].append(train_acc)\n",
    "        finetune_history['test_loss'].append(test_loss)\n",
    "        finetune_history['test_acc'].append(test_acc)\n",
    "        \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "        \n",
    "        # Print progress (every 10 epochs or last epoch)\n",
    "        if ft_epoch % 10 == 0 or ft_epoch == FINETUNE_EPOCHS - 1:\n",
    "            print(f\"Epoch {global_epoch:3d} | LR: {current_lr:.6f} | \"\n",
    "                  f\"Train: {train_acc:.2f}% (Loss: {train_loss:.4f}) | \"\n",
    "                  f\"Test: {test_acc:.2f}% | Best: {best_test_acc:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINE-TUNING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Best test accuracy: {best_test_acc:.2f}%\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9e9c3",
   "metadata": {},
   "source": [
    "## Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eb_ticket is not None and 'finetune_history' in dir():\n",
    "    # Combined plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Full training trajectory\n",
    "    ax1 = axes[0]\n",
    "    all_epochs = history['epoch'] + finetune_history['epoch']\n",
    "    all_train_acc = history['train_acc'] + finetune_history['train_acc']\n",
    "    all_test_acc = history['test_acc'] + finetune_history['test_acc']\n",
    "    \n",
    "    ax1.plot(all_epochs, all_train_acc, label='Train Acc', color='#2E86AB', linewidth=2, alpha=0.7)\n",
    "    ax1.plot(all_epochs, all_test_acc, label='Test Acc', color='#F18F01', linewidth=2, alpha=0.9)\n",
    "    ax1.axvline(x=convergence_epoch, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'EB Ticket @ epoch {convergence_epoch}')\n",
    "    ax1.fill_between([convergence_epoch, max(all_epochs)], 0, 100, \n",
    "                     alpha=0.1, color='green', label='Fine-tuning phase')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax1.set_title('Complete Training Trajectory', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 100])\n",
    "    \n",
    "    # Mask distance with convergence visualization\n",
    "    ax2 = axes[1]\n",
    "    valid_distances = [(e, d) for e, d in zip(history['epoch'], history['mask_distance']) if d >= 0]\n",
    "    if valid_distances:\n",
    "        epochs_valid, dists_valid = zip(*valid_distances)\n",
    "        ax2.semilogy(epochs_valid, dists_valid, 'o-', color='#2E86AB', markersize=5, \n",
    "                     linewidth=2, label='Mask Distance')\n",
    "        ax2.axhline(y=DISTANCE_THRESHOLD, color='#A23B72', linestyle='--', linewidth=2,\n",
    "                    label=f'Threshold Œµ={DISTANCE_THRESHOLD}')\n",
    "        ax2.axvline(x=convergence_epoch, color='red', linestyle='--', linewidth=2,\n",
    "                    label=f'Convergence @ epoch {convergence_epoch}')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Hamming Distance (log scale)', fontsize=12)\n",
    "    ax2.set_title('Mask Convergence', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ROOT_DIR / 'results' / 'earlybird_vgg16_cifar10_complete.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úì Figure saved to results/earlybird_vgg16_cifar10_complete.png\")\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EARLY-BIRD EXPERIMENT SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Model: VGG16-BN\")\n",
    "    print(f\"Dataset: CIFAR-10\")\n",
    "    print(f\"Target Sparsity: {TARGET_SPARSITY * 100:.0f}%\")\n",
    "    print(f\"Early-Bird Ticket Found: Epoch {convergence_epoch}\")\n",
    "    print(f\"Total Training Epochs (Search + Fine-tune): {max(all_epochs) + 1}\")\n",
    "    print(f\"Training Epochs Saved: {TOTAL_EPOCHS - convergence_epoch - 1}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Accuracy at EB discovery: {history['test_acc'][-1]:.2f}%\")\n",
    "    print(f\"Final Test Accuracy: {finetune_history['test_acc'][-1]:.2f}%\")\n",
    "    print(f\"Best Test Accuracy: {best_test_acc:.2f}%\")\n",
    "    print(f\"Actual Channel Sparsity: {get_overall_channel_sparsity(eb_ticket) * 100:.2f}%\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Summary statistics table\n",
    "    print(f\"\\n{'Metric':<35} {'Value':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Early-Bird Discovery Epoch':<35} {convergence_epoch:<15}\")\n",
    "    print(f\"{'Search Phase Test Acc':<35} {history['test_acc'][-1]:.2f}%\")\n",
    "    print(f\"{'After Pruning (before FT)':<35} {test_acc_pruned:.2f}%\")\n",
    "    print(f\"{'Final Test Acc (after FT)':<35} {finetune_history['test_acc'][-1]:.2f}%\")\n",
    "    print(f\"{'Best Test Acc':<35} {best_test_acc:.2f}%\")\n",
    "    print(f\"{'Accuracy Drop from Search':<35} {(history['test_acc'][-1] - finetune_history['test_acc'][-1]):.2f}%\")\n",
    "    print(f\"{'Channel Sparsity':<35} {get_overall_channel_sparsity(eb_ticket) * 100:.2f}%\")\n",
    "    print(f\"{'Training Epochs Saved':<35} {TOTAL_EPOCHS - convergence_epoch - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55f891",
   "metadata": {},
   "source": [
    "## Quick Test Mode (Optional)\n",
    "\n",
    "Run a shorter test with reduced epochs to verify the implementation works correctly before full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with minimal epochs to verify implementation\n",
    "# Set QUICK_TEST = True to run a fast verification\n",
    "\n",
    "QUICK_TEST = False  # Change to True for quick verification\n",
    "\n",
    "if QUICK_TEST:\n",
    "    print(\"Running quick test mode (5 epochs)...\")\n",
    "    \n",
    "    # Small model for quick test\n",
    "    quick_model = VGG16_BN(num_classes=10).to(device)\n",
    "    quick_optimizer = optim.SGD(quick_model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "    quick_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    quick_finder = EarlyBirdFinder(\n",
    "        target_sparsity=0.5,\n",
    "        patience=2,  # Lower patience for quick test\n",
    "        distance_threshold=0.2,  # Higher threshold for quick test\n",
    "        pruning_method='global'\n",
    "    )\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        train_loss, l1_loss, train_acc = train_epoch_with_l1(\n",
    "            quick_model, train_loader, quick_optimizer, quick_criterion, 1e-4, device\n",
    "        )\n",
    "        test_loss, test_acc = evaluate(quick_model, test_loader, quick_criterion, device)\n",
    "        converged, dist = quick_finder.record_epoch(quick_model, epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train {train_acc:.2f}% | Test {test_acc:.2f}% | Dist {dist:.4f}\")\n",
    "        \n",
    "        if converged:\n",
    "            print(f\"‚úì Quick test: EB ticket found at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n‚úì Quick test completed successfully!\")\n",
    "    print(\"The implementation is working. Set QUICK_TEST = False and re-run for full training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lth_thesis-3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
