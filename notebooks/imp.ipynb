{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d713a8e",
   "metadata": {},
   "source": [
    "# Lottery Ticket Hypothesis - LeNet-300-100 on MNIST\n",
    "\n",
    "This notebook implements the Lottery Ticket Hypothesis experiment with:\n",
    "- **Model**: LeNet-300-100 (Fully connected: 784→300→100→10)\n",
    "- **Dataset**: MNIST\n",
    "- **Optimizer**: Adam (lr=1.2e-3)\n",
    "- **Batch Size**: 60\n",
    "- **Initialization**: Gaussian Glorot (Xavier)\n",
    "- **Pruning Strategy**: Unstructured, layer-wise magnitude pruning\n",
    "- **Pruning Rates**: 20% for hidden layers, 10% for output layer\n",
    "- **Total Iterations**: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad2234",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2ecda8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T03:00:09.296248Z",
     "start_time": "2026-01-12T03:00:04.066138Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from pruning import prune_by_percent, create_initial_masks, get_sparsity, get_overall_sparsity\n",
    "from train import train_epoch, evaluate, train_iterations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "43bc9795",
   "metadata": {},
   "source": [
    "## Define LeNet-300-100 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fded4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet300100(nn.Module):\n",
    "    \"\"\"LeNet-300-100: Fully connected network with 300 and 100 hidden units\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet300100, self).__init__()\n",
    "        # 784 (28x28) -> 300 -> 100 -> 10\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "        \n",
    "        # Initialize with Gaussian Glorot (Xavier)\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with Xavier/Glorot normal initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Hidden layers with ReLU activation\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        # Output layer (no activation, use with CrossEntropyLoss)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def apply_mask(self, masks):\n",
    "        \"\"\"Apply pruning masks to the model weights\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight.data *= torch.from_numpy(masks['fc1.weight']).float()\n",
    "            self.fc2.weight.data *= torch.from_numpy(masks['fc2.weight']).float()\n",
    "            self.fc3.weight.data *= torch.from_numpy(masks['fc3.weight']).float()\n",
    "    \n",
    "    def get_weights_as_dict(self):\n",
    "        \"\"\"Get model weights as numpy dictionary\"\"\"\n",
    "        return {\n",
    "            'fc1.weight': self.fc1.weight.data.cpu().numpy(),\n",
    "            'fc2.weight': self.fc2.weight.data.cpu().numpy(),\n",
    "            'fc3.weight': self.fc3.weight.data.cpu().numpy()\n",
    "        }\n",
    "    \n",
    "    def set_weights_from_dict(self, weights):\n",
    "        \"\"\"Set model weights from numpy dictionary\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight.data = torch.from_numpy(weights['fc1.weight']).float()\n",
    "            self.fc2.weight.data = torch.from_numpy(weights['fc2.weight']).float()\n",
    "            self.fc3.weight.data = torch.from_numpy(weights['fc3.weight']).float()\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = LeNet300100().to(device)\n",
    "print(f\"\\nModel architecture:\\n{model}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f87b8",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0bffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='../data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders with batch size 60\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=60,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1000,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e0f0b",
   "metadata": {},
   "source": [
    "## Configure Training and Pruning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0764ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    # Training parameters\n",
    "    'learning_rate': 1.2e-3,\n",
    "    'batch_size': 60,\n",
    "    'total_iterations': 100,\n",
    "    \n",
    "    # Pruning parameters\n",
    "    'prune_percents': {\n",
    "        'fc1.weight': 0.20,  # 20% for first hidden layer\n",
    "        'fc2.weight': 0.20,  # 20% for second hidden layer\n",
    "        'fc3.weight': 0.10,  # 10% for output layer\n",
    "    },\n",
    "    \n",
    "    # Number of pruning iterations\n",
    "    'num_pruning_iterations': 20,  # Will prune iteratively\n",
    "    \n",
    "    # Calculate training iterations per pruning round\n",
    "    'iterations_per_round': None  # Will calculate below\n",
    "}\n",
    "\n",
    "# Calculate iterations per pruning round\n",
    "config['iterations_per_round'] = config['total_iterations'] // config['num_pruning_iterations']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"Learning Rate: {config['learning_rate']}\")\n",
    "print(f\"Batch Size: {config['batch_size']}\")\n",
    "print(f\"Initialization: Gaussian Glorot (Xavier)\")\n",
    "print(f\"\\nPruning Strategy: Unstructured, layer-wise magnitude pruning\")\n",
    "print(f\"Pruning Rate - Hidden Layers: {config['prune_percents']['fc1.weight']*100}%\")\n",
    "print(f\"Pruning Rate - Output Layer: {config['prune_percents']['fc3.weight']*100}%\")\n",
    "print(f\"\\nTotal Training Iterations: {config['total_iterations']:,}\")\n",
    "print(f\"Number of Pruning Rounds: {config['num_pruning_iterations']}\")\n",
    "print(f\"Training Iterations per Round: {config['iterations_per_round']:,}\")\n",
    "print(f\"\\nResetting: Weights reset to initial values (θ₀)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356952ca",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e793d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions are imported from src/train.py\n",
    "# Available functions:\n",
    "#   - train_epoch(model, train_loader, criterion, optimizer, device, masks, apply_mask_fn)\n",
    "#   - evaluate(model, test_loader, criterion, device)\n",
    "#   - train_iterations(model, train_loader, test_loader, criterion, optimizer, \n",
    "#                      num_iterations, device, masks, eval_freq, apply_mask_fn, verbose)\n",
    "\n",
    "print(\"Training functions imported from src/train.py\")\n",
    "print(\"  - train_epoch: Train for one epoch\")\n",
    "print(\"  - evaluate: Evaluate model on test set\")  \n",
    "print(\"  - train_iterations: Train for N iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9433ad1",
   "metadata": {},
   "source": [
    "## Initialize Model and Save Initial Weights (θ₀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize model to get fresh weights\n",
    "model = LeNet300100().to(device)\n",
    "\n",
    "# Save initial weights (θ₀) - these will be used for rewinding\n",
    "initial_weights = model.get_weights_as_dict()\n",
    "print(\"Initial weights (θ₀) saved for lottery ticket rewinding.\")\n",
    "\n",
    "# Create initial masks (all ones - no pruning yet)\n",
    "masks = create_initial_masks(initial_weights)\n",
    "print(f\"\\nInitial masks created:\")\n",
    "for layer, mask in masks.items():\n",
    "    print(f\"  {layer}: shape {mask.shape}, sparsity: 0.00%\")\n",
    "\n",
    "# Setup loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "print(f\"\\nOptimizer: Adam(lr={config['learning_rate']})\")\n",
    "print(\"Model ready for Iterative Magnitude Pruning (IMP)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc33429",
   "metadata": {},
   "source": [
    "## Iterative Magnitude Pruning (IMP) Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ed9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for results across pruning iterations\n",
    "results = {\n",
    "    'pruning_iteration': [],\n",
    "    'overall_sparsity': [],\n",
    "    'layer_sparsity': [],\n",
    "    'test_accuracy': [],\n",
    "    'training_history': []\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING ITERATIVE MAGNITUDE PRUNING (IMP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for pruning_iter in range(config['num_pruning_iterations']):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PRUNING ITERATION {pruning_iter + 1}/{config['num_pruning_iterations']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Calculate current sparsity\n",
    "    overall_sparsity = get_overall_sparsity(masks)\n",
    "    layer_sparsity = get_sparsity(masks)\n",
    "    \n",
    "    print(f\"\\nCurrent Network Sparsity:\")\n",
    "    print(f\"  Overall: {overall_sparsity*100:.2f}%\")\n",
    "    for layer, sparsity in layer_sparsity.items():\n",
    "        remaining = (1 - sparsity) * 100\n",
    "        print(f\"  {layer}: {sparsity*100:.2f}% pruned ({remaining:.2f}% remaining)\")\n",
    "    \n",
    "    # Reset model to initial weights (θ₀) with current mask\n",
    "    model.set_weights_from_dict(initial_weights)\n",
    "    model.apply_mask(masks)\n",
    "    print(f\"\\n✓ Weights reset to initial values (θ₀) with pruning mask applied\")\n",
    "    \n",
    "    # Reinitialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Train for specified iterations\n",
    "    print(f\"\\nTraining for {config['iterations_per_round']:,} iterations...\")\n",
    "    training_history = train_iterations(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_iterations=config['iterations_per_round'],\n",
    "        device=device,\n",
    "        masks=masks,\n",
    "        eval_freq=500\n",
    "    )\n",
    "    \n",
    "    # Get final test accuracy\n",
    "    final_test_acc = training_history['final_test_acc']\n",
    "    print(f\"\\n✓ Training complete. Final test accuracy: {final_test_acc:.2f}%\")\n",
    "    \n",
    "    # Store results\n",
    "    results['pruning_iteration'].append(pruning_iter)\n",
    "    results['overall_sparsity'].append(overall_sparsity)\n",
    "    results['layer_sparsity'].append(layer_sparsity.copy())\n",
    "    results['test_accuracy'].append(final_test_acc)\n",
    "    results['training_history'].append(training_history)\n",
    "    \n",
    "    # Prune for next iteration (if not the last iteration)\n",
    "    if pruning_iter < config['num_pruning_iterations'] - 1:\n",
    "        print(f\"\\nPruning {config['prune_percents']['fc1.weight']*100:.0f}% of remaining weights in hidden layers...\")\n",
    "        print(f\"Pruning {config['prune_percents']['fc3.weight']*100:.0f}% of remaining weights in output layer...\")\n",
    "        \n",
    "        # Get current weights\n",
    "        current_weights = model.get_weights_as_dict()\n",
    "        \n",
    "        # Apply magnitude-based pruning using the pruning function from src/pruning.py\n",
    "        masks = prune_by_percent(\n",
    "            percents=config['prune_percents'],\n",
    "            masks=masks,\n",
    "            final_weights=current_weights\n",
    "        )\n",
    "        print(\"✓ Pruning complete. Updated masks for next iteration.\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ITERATIVE MAGNITUDE PRUNING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nFinal Network Statistics:\")\n",
    "print(f\"  Total pruning iterations: {config['num_pruning_iterations']}\")\n",
    "print(f\"  Final overall sparsity: {results['overall_sparsity'][-1]*100:.2f}%\")\n",
    "print(f\"  Final test accuracy: {results['test_accuracy'][-1]:.2f}%\")\n",
    "print(f\"  Best test accuracy: {max(results['test_accuracy']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24cac3",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test accuracy vs sparsity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Test Accuracy vs Sparsity\n",
    "sparsities = [s * 100 for s in results['overall_sparsity']]\n",
    "accuracies = results['test_accuracy']\n",
    "\n",
    "ax1.plot(sparsities, accuracies, 'o-', linewidth=2, markersize=8, color='#2E86AB')\n",
    "ax1.set_xlabel('Overall Sparsity (%)', fontsize=12)\n",
    "ax1.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Lottery Ticket Hypothesis: Test Accuracy vs Sparsity', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([min(accuracies) - 2, max(accuracies) + 2])\n",
    "\n",
    "# Annotate key points\n",
    "max_acc_idx = accuracies.index(max(accuracies))\n",
    "ax1.annotate(f'Best: {max(accuracies):.2f}%\\n@ {sparsities[max_acc_idx]:.1f}% sparsity',\n",
    "            xy=(sparsities[max_acc_idx], accuracies[max_acc_idx]),\n",
    "            xytext=(10, -30), textcoords='offset points',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
    "            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "# Plot 2: Layer-wise Sparsity Evolution\n",
    "ax2.set_xlabel('Pruning Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Sparsity (%)', fontsize=12)\n",
    "ax2.set_title('Layer-wise Sparsity Evolution', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "iterations = list(range(len(results['layer_sparsity'])))\n",
    "layers = list(results['layer_sparsity'][0].keys())\n",
    "colors = ['#A23B72', '#F18F01', '#2E86AB']\n",
    "\n",
    "for i, layer in enumerate(layers):\n",
    "    layer_sparsities = [results['layer_sparsity'][j][layer] * 100 \n",
    "                        for j in range(len(results['layer_sparsity']))]\n",
    "    ax2.plot(iterations, layer_sparsities, 'o-', linewidth=2, \n",
    "            markersize=6, label=layer, color=colors[i])\n",
    "\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"{'Iteration':<12} {'Sparsity':<12} {'Test Acc':<12}\")\n",
    "print(\"-\" * 36)\n",
    "for i in range(len(results['pruning_iteration'])):\n",
    "    print(f\"{i:<12} {sparsities[i]:<11.2f}% {accuracies[i]:<11.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lth-3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
